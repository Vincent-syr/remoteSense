{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "import copy\n",
    "from methods import backbone, resnet12\n",
    "from data.datamgr import SetDataManager\n",
    "# from methods.protonet import ProtoNet\n",
    "from methods.protonet_multi_gpu import ProtoNetMulti\n",
    "from loss.nt_xent import NTXentLoss\n",
    "from io_utils import model_dict, parse_args, get_resume_file, get_trlog, save_fig\n",
    "from utils import Timer\n",
    "import argparse\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(dataset='NWPU', init_lr=0.001, lr_anneal='const', method='cs_protonet', mlp_dropout=0.7, model='ResNet12', n_episode=100, n_query=8, n_shot=1, num_classes=200, optim='Adam', os='linux', save_freq=10, start_epoch=0, stop_epoch=300, test_n_way=5, train_aug=True, train_n_way=5, warmup=False, wd='0')\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--dataset'     , default='CUB',        help='CUB/miniImagenet/cross/omniglot/cross_char')\n",
    "parser.add_argument('--os'          , default='linux',        help='linux/windows')\n",
    "\n",
    "parser.add_argument('--dataset'     , default='NWPU',        help='NPPU/WHU-RS19/UCMERCED')\n",
    "parser.add_argument('--model'       , default='ResNet12',      help='model: Conv{4|6} / ResNet{10|18|34|50|101}') # 50 and 101 are not used in the paper\n",
    "parser.add_argument('--method'      , default='cs_protonet',   help='rotate/baseline/baseline++/protonet/matchingnet/relationnet{_softmax}/maml{_approx}') #relationnet_softmax replace L2 norm with softmax to expedite training, maml_approx use first-order approximation in the gradient for efficiency\n",
    "parser.add_argument('--train_n_way' , default=5, type=int,  help='class num to classify for training') #baseline and baseline++ would ignore this parameter\n",
    "parser.add_argument('--test_n_way'  , default=5, type=int,  help='class num to classify for testing (validation) ') #baseline and baseline++ only use this parameter in finetuning\n",
    "parser.add_argument('--n_shot'      , default=1, type=int,  help='number of labeled data in each class, same as n_support') #baseline and baseline++ only use this parameter in finetuning\n",
    "parser.add_argument('--n_query'      , default=8, type=int,  help='number of unlabeled  query data in each class, same as n_query') #baseline and baseline++ only use this parameter in finetuning\n",
    "\n",
    "parser.add_argument('--train_aug'   , default=True, type=bool, help='perform data augmentation or not during training ') #still required for save_features.py and test.py to find the model path correctly\n",
    "# parser.add_argument('--no_aug' ,dest='train_aug', action='store_false', default=True,  help='perform data augmentation or not during training ') \n",
    "\n",
    "parser.add_argument('--n_episode', default=100, type=int, help = 'num of episodes in each epoch')\n",
    "parser.add_argument('--mlp_dropout' , default=0.7, help='dropout rate in word embedding transformer')\n",
    "# parser.add_argument('--aux'   , default=False,  help='use attribute as auxiliary data, multimodal method')\n",
    "\n",
    "# learning rate, optim\n",
    "parser.add_argument('--lr_anneal', default='const', help='const/pwc/exp, schedule learning rate')\n",
    "parser.add_argument('--init_lr', default=0.001)\n",
    "parser.add_argument('--optim', default='Adam', help='Adam/SGD')\n",
    "parser.add_argument('--wd', default='0', help='weight_decay  /  {0|0.001|...}')\n",
    "\n",
    "parser.add_argument('--num_classes' , default=200, type=int, help='total number of classes in softmax, only used in baseline') #make it larger than the maximum label value in base class\n",
    "parser.add_argument('--save_freq'   , default=10, type=int, help='Save frequency')\n",
    "parser.add_argument('--start_epoch' , default=0, type=int,help ='Starting epoch')\n",
    "parser.add_argument('--stop_epoch'  , default=300, type=int, help ='Stopping epoch') #for meta-learning methods, each epoch contains 100 episodes. The default epoch number is dataset dependent. See train.py\n",
    "parser.add_argument('--warmup'      , action='store_true', help='continue from baseline, neglected if resume is true') #never used in the paper\n",
    "\n",
    "\n",
    "\n",
    "params = parser.parse_args([])\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(dataset='NWPU', init_lr=0.001, lr_anneal='const', method='cs_protonet', mlp_dropout=0.7, model='ResNet12', n_episode=100, n_query=8, n_shot=1, num_classes=200, optim='Adam', os='linux', save_freq=10, start_epoch=0, stop_epoch=300, test_n_way=5, train_aug=True, train_n_way=5, warmup=False, wd='0')\n",
      "image_size =  84\n",
      "n_query =  8\n",
      "checkpoint_dir =  checkpoints/NWPU/ResNet12_cs_protonet_aug_Adam_lr0.001_const_wd0_5way_1shot\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "print(params)    \n",
    "\n",
    "# if params.os == 'linux':\n",
    "#     base_file = os.path.join('./filelists', params.dataset, 'base_linux.json')\n",
    "#     val_file = os.path.join('./filelists', params.dataset, 'val_linux.json')\n",
    "# else:\n",
    "base_file = os.path.join('./filelists', params.dataset, ('base_%s.json' % params.os))\n",
    "val_file = os.path.join('./filelists', params.dataset,  ('val_%s.json' % params.os))\n",
    "# novel_file = os.path.join('./filelists', params.dataset, 'novel.json')\n",
    "\n",
    "image_size = 84\n",
    "print('image_size = ', image_size)\n",
    "print(\"n_query = \", params.n_query)\n",
    "params.batch_size = (params.n_query + params.n_shot) * params.train_n_way\n",
    "\n",
    "train_few_shot_params   = dict(n_way = params.train_n_way, n_support = params.n_shot, n_query=params.n_query) \n",
    "base_datamgr            = SetDataManager(image_size, n_episode=params.n_episode, params=params, **train_few_shot_params)\n",
    "base_loader             = base_datamgr.get_data_loader( base_file , aug = params.train_aug )\n",
    "\n",
    "test_few_shot_params     = dict(n_way = params.test_n_way, n_support = params.n_shot, n_query = params.n_query) \n",
    "val_datamgr             = SetDataManager(image_size, n_episode=params.n_episode, params=params, **test_few_shot_params)\n",
    "val_loader              = val_datamgr.get_data_loader(val_file, aug = False) \n",
    "\n",
    "model = ProtoNetMulti(model_dict[params.model], params=params, **train_few_shot_params)\n",
    "model = model.cuda()\n",
    "params.checkpoint_dir = 'checkpoints/%s/%s_%s' %(params.dataset, params.model, params.method)\n",
    "if params.train_aug:\n",
    "    params.checkpoint_dir += '_aug'\n",
    "\n",
    "params.checkpoint_dir += '_%s_lr%s_%s_wd%s' % (params.optim, str(params.init_lr), params.lr_anneal, str(params.wd))\n",
    "\n",
    "if not params.method  in ['baseline', 'baseline++']: \n",
    "    params.checkpoint_dir += '_%dway_%dshot' %( params.train_n_way, params.n_shot)\n",
    "params.model_dir = os.path.join(params.checkpoint_dir, 'model')\n",
    "if not os.path.isdir(params.model_dir):\n",
    "    os.makedirs(params.model_dir)\n",
    "print('checkpoint_dir = ', params.checkpoint_dir)\n",
    "start_epoch = params.start_epoch\n",
    "stop_epoch = params.stop_epoch\n",
    "max_acc = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train contrastive learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_mlp = nn.Sequential(\n",
    "    nn.Linear(model.feat_dim, model.feat_dim),\n",
    "    nn.ReLU(), \n",
    "    nn.Linear(model.feat_dim, 256)\n",
    ").cuda()\n",
    "\n",
    "cs_criterion = NTXentLoss(params.batch_size).cuda()\n",
    "init_lr = params.init_lr\n",
    "optimizer = torch.optim.Adam([\n",
    "            {'params': model.parameters()},\n",
    "            {'params': cs_mlp.parameters()}\n",
    "    ], lr=init_lr)\n",
    "\n",
    "timer = Timer()\n",
    "print_freq = 50\n",
    "lossfn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProtoNetMulti(\n",
       "  (feature): ResNet(\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): LeakyReLU(negative_slope=0.1)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (DropBlock): DropBlock()\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): LeakyReLU(negative_slope=0.1)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (DropBlock): DropBlock()\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): LeakyReLU(negative_slope=0.1)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (DropBlock): DropBlock()\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): LeakyReLU(negative_slope=0.1)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (DropBlock): DropBlock()\n",
       "      )\n",
       "    )\n",
       "    (fc): Linear(in_features=512, out_features=64, bias=True)\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, (x, _) in enumerate(base_loader, 1):\n",
    "    xi = x[0].cuda()\n",
    "    xj = x[1].cuda()\n",
    "    # compute clf loss and acc\n",
    "    zi = model.forward(xi)   # 只算xi的loss\n",
    "    zj = model.forward(xj)  \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.compute_score(zi)\n",
    "correct_this, count_this = model.correct(scores)\n",
    "y_query = torch.from_numpy(np.repeat(range(model.n_way), model.n_query))\n",
    "y_query = Variable(y_query.long().cuda())\n",
    "clf_loss = model.loss_fn(scores, y_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute contrastive loss\n",
    "zics = cs_mlp(zi)\n",
    "zjcs = cs_mlp(zj)\n",
    "cs_loss = cs_criterion(zics, zjcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 40\n",
      "tensor(35.4992, device='cuda:0', grad_fn=<NllLossBackward>) tensor(4.2756, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(correct_this, count_this)\n",
    "print(clf_loss, cs_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch140-gpu",
   "language": "python",
   "name": "torch140-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
